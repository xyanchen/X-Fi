{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from torch import nn\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import yaml\n",
    "from evaluate import error\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "from torchvision.transforms import Resize\n",
    "from syn_DI_dataset import make_dataset, make_dataloader\n",
    "\n",
    "from RGB_benchmark.rgb_ResNet18.RGB_ResNet import *\n",
    "from depth_benchmark.depth_ResNet18 import *\n",
    "from mmwave_benchmark.mmwave_point_transformer import *\n",
    "from lidar_benchmark.lidar_point_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rgb_feature_extractor(nn.Module):\n",
    "    def __init__(self, rgb_model):\n",
    "        super(rgb_feature_extractor, self).__init__()\n",
    "        self.part = nn.Sequential(*list(rgb_model.children())[:-2])\n",
    "    def forward(self, x):\n",
    "        x = self.part(x).view(x.size(0), 512, -1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class depth_feature_extractor(nn.Module):\n",
    "    def __init__(self, depth_model):\n",
    "        super(depth_feature_extractor, self).__init__()\n",
    "        self.part = nn.Sequential(*list(depth_model.children())[:-2])\n",
    "    def forward(self, x):\n",
    "        x = self.part(x).view(x.size(0), 512, -1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mmwave_feature_extractor(nn.Module):\n",
    "    def __init__(self, mmwave_model):\n",
    "        super(mmwave_feature_extractor, self).__init__()\n",
    "        self.part = nn.Sequential(*list(mmwave_model.children())[:-1])\n",
    "    def forward(self, x):\n",
    "        x, _ = self.part(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lidar_feature_extractor(nn.Module):\n",
    "    def __init__(self, lidar_model):\n",
    "        super(lidar_feature_extractor, self).__init__()\n",
    "        # self.model = lidar_model\n",
    "        npoints, nblocks, nneighbor, n_c, d_points = 1024, 5, 16, 51, 3\n",
    "        self.fc1 = lidar_model.backbone.fc1\n",
    "        self.transformer1 = lidar_model.backbone.transformer1\n",
    "        self.transition_downs = nn.ModuleList()\n",
    "        self.transformers = nn.ModuleList()\n",
    "        for i in range(nblocks - 4):\n",
    "            channel = 32 * 2 ** (i + 1)\n",
    "            self.transition_downs.append(lidar_model.backbone.transition_downs[i])\n",
    "            self.transformers.append(lidar_model.backbone.transformers[i])\n",
    "        self.nblocks = nblocks\n",
    "    \n",
    "    def forward(self, x):\n",
    "        xyz = x[..., :3]\n",
    "        points = self.transformer1(xyz, self.fc1(x))[0]\n",
    "\n",
    "        xyz_and_feats = [(xyz, points)]\n",
    "        for i in range(self.nblocks - 4):\n",
    "            xyz, points = self.transition_downs[i](xyz, points)\n",
    "            points = self.transformers[i](xyz, points)[0]\n",
    "            xyz_and_feats.append((xyz, points))\n",
    "        points = points.view(points.size(0), -1, 512)\n",
    "        return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class csi_feature_extractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(csi_feature_extractor, self).__init__()\n",
    "        self.part = nn.Sequential(\n",
    "            model.encoder_conv1,\n",
    "            model.encoder_bn1,\n",
    "            model.encoder_relu,\n",
    "            model.encoder_layer1,\n",
    "            model.encoder_layer2,\n",
    "            model.encoder_layer3,\n",
    "            model.encoder_layer4, \n",
    "            # torch.nn.AvgPool2d((1, 4))\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = torch.transpose(x, 2, 3) #16,2,114,3,32\n",
    "        x = torch.flatten(x, 3, 4)# 16,2,114,96\n",
    "        torch_resize = Resize([136,32])\n",
    "        x = torch_resize(x)\n",
    "        x = self.part(x).view(x.size(0), 512, -1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_extrator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(feature_extrator, self).__init__()\n",
    "        \n",
    "        rgb_model = RGB_ResNet18()\n",
    "        rgb_model.load_state_dict(torch.load('./RGB_benchmark/rgb_ResNet18/RGB_Resnet18_copy.pt'))\n",
    "        rgb_extractor = rgb_feature_extractor(rgb_model)\n",
    "        rgb_extractor.eval()\n",
    "\n",
    "        depth_model = Depth_ResNet18()\n",
    "        depth_model.load_state_dict(torch.load('depth_benchmark/depth_Resnet18.pt'))\n",
    "        depth_extractor = depth_feature_extractor(depth_model)\n",
    "        depth_extractor.eval()\n",
    "        \n",
    "        mmwave_model = mmwave_PointTransformerReg()\n",
    "        mmwave_model.load_state_dict(torch.load('mmwave_benchmark/mmwave_all_random.pt'))\n",
    "        mmwave_extractor = mmwave_feature_extractor(mmwave_model)\n",
    "        mmwave_extractor.eval()\n",
    "\n",
    "        lidar_model = lidar_PointTransformerReg()\n",
    "        lidar_model.load_state_dict(torch.load('lidar_benchmark/lidar_all_random.pt'))\n",
    "        lidar_extractor = lidar_feature_extractor(lidar_model)\n",
    "        lidar_extractor.eval()\n",
    "\n",
    "        sys.path.insert(0, './CSI_benchmark')\n",
    "        csi_model = torch.load('CSI_benchmark/protocol3_random_1.pkl')\n",
    "        csi_extractor = csi_feature_extractor(csi_model)\n",
    "        csi_extractor.eval()\n",
    "\n",
    "        self.rgb_extractor = rgb_extractor\n",
    "        self.depth_extractor = depth_extractor\n",
    "        self.mmwave_extractor = mmwave_extractor\n",
    "        self.lidar_extractor = lidar_extractor\n",
    "        self.csi_extractor = csi_extractor\n",
    "        \n",
    "    def forward(self, rgb_data, depth_data, mmwave_data, lidar_data, csi_data):\n",
    "        rgb_feature = self.rgb_extractor(rgb_data)\n",
    "        depth_feature = self.depth_extractor(depth_data)\n",
    "        mmwave_feature = self.mmwave_extractor(mmwave_data)\n",
    "        lidar_feature = self.lidar_extractor(lidar_data)\n",
    "        csi_feature = self.csi_extractor(csi_data)\n",
    "        return rgb_feature, depth_feature, mmwave_feature, lidar_feature, csi_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_projector(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(linear_projector, self).__init__()\n",
    "        self.linear_projection = nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, rgb_feature, depth_feature, mmwave_feature, lidar_feature, csi_feature, modality_list):\n",
    "        feature_list = [rgb_feature, depth_feature, mmwave_feature, lidar_feature, csi_feature]\n",
    "        if sum (modality_list) == 0:\n",
    "            print('WARNING: modality_list is empty!')\n",
    "            feature = torch.zeros_like(lidar_feature, device=torch.device('cuda'))\n",
    "            feature = self.linear_projection(feature)\n",
    "        else:\n",
    "            real_feature_list = []\n",
    "            for i in range(len(modality_list)):\n",
    "                if modality_list[i] == True:\n",
    "                    real_feature_list.append(feature_list[i])\n",
    "                else:\n",
    "                    continue\n",
    "            feature = torch.cat(real_feature_list, dim=1)\n",
    "            feature = self.linear_projection(feature)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: modality_list is empty!\n"
     ]
    }
   ],
   "source": [
    "# modality_list = [True, True, True, True, True]\n",
    "modality_list = [False,False,False,False,False]\n",
    "if sum(modality_list) == 0:\n",
    "    print('WARNING: modality_list is empty!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size = 256, num_heads = 4, dropout = 0.0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = nn.Linear(emb_size, emb_size*3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "        \n",
    "        scaling = self.emb_size ** (1/2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion = 4, drop_p = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "        \n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size = 256,\n",
    "                 drop_p = 0.5,\n",
    "                 forward_expansion = 4,\n",
    "                 forward_drop_p = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "        \n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth = 1, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "        \n",
    "class regression_Head(nn.Sequential):\n",
    "    def __init__(self, emb_size, num_classes):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, num_classes))\n",
    "        \n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                emb_size = 256,\n",
    "                depth = 1,\n",
    "                *,\n",
    "                num_classes = 17*3,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            regression_Head(emb_size, num_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modality_invariant_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(modality_invariant_model, self).__init__()\n",
    "        self.feature_extractor = feature_extrator()\n",
    "        self.linear_projector = linear_projector(512, 256)\n",
    "        self.vit = ViT()\n",
    "    def forward(self, rgb_data, depth_data, mmwave_data, lidar_data, csi_data, modality_list):\n",
    "        rgb_feature, depth_feature, mmwave_feature, lidar_feature, csi_feature = self.feature_extractor(rgb_data, depth_data, mmwave_data, lidar_data, csi_data)\n",
    "        feature = self.linear_projector(rgb_feature, depth_feature, mmwave_feature, lidar_feature, csi_feature, modality_list)\n",
    "        out = self.vit(feature)\n",
    "        out = out.view(-1, 17, 3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: modality_list is empty!\n",
      "torch.Size([32, 17, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rgb_data = torch.rand(32, 3, 480, 640).cuda()\n",
    "depth_data = torch.rand(32, 3, 480, 640).cuda()\n",
    "mmwave_data = torch.rand(32, 31, 5).cuda()\n",
    "lidar_data = torch.rand(32, 1467, 3).cuda()\n",
    "csi_data = torch.rand(32, 3, 114, 10).cuda()\n",
    "# modality_list = random.choices(\n",
    "#     [True, False],\n",
    "#     k= 5,\n",
    "#     weights=[80, 20]\n",
    "# )\n",
    "# modality_list = [True, True, True, True, True]\n",
    "modality_list = [False,False,False,False,False]\n",
    "model = modality_invariant_model().cuda()\n",
    "out = model(rgb_data, depth_data, mmwave_data, lidar_data, csi_data, modality_list)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modality_invariant_model(\n",
      "  (feature_extractor): feature_extrator(\n",
      "    (rgb_extractor): rgb_feature_extractor(\n",
      "      (part): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(3, 3, kernel_size=(14, 14), stride=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(3, 3, kernel_size=(5, 56), stride=(1, 1))\n",
      "          (3): ReLU()\n",
      "          (4): Conv2d(3, 3, kernel_size=(5, 23), stride=(1, 1))\n",
      "          (5): ReLU()\n",
      "          (6): Conv2d(3, 16, kernel_size=(3, 14), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2d(16, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU()\n",
      "        (4): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (5): Sequential(\n",
      "          (0): Block(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "          (1): Block(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): Block(\n",
      "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (i_downsample): Sequential(\n",
      "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "          (1): Block(\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): Block(\n",
      "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (i_downsample): Sequential(\n",
      "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "          (1): Block(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (8): Sequential(\n",
      "          (0): Block(\n",
      "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (batch_norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (i_downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "          (1): Block(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (depth_extractor): depth_feature_extractor(\n",
      "      (part): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(3, 3, kernel_size=(14, 14), stride=(2, 2))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(3, 3, kernel_size=(5, 56), stride=(1, 1))\n",
      "          (3): ReLU()\n",
      "          (4): Conv2d(3, 3, kernel_size=(5, 23), stride=(1, 1))\n",
      "          (5): ReLU()\n",
      "          (6): Conv2d(3, 16, kernel_size=(3, 14), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2d(16, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU()\n",
      "        (4): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (5): Sequential(\n",
      "          (0): Block(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "          (1): Block(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): Block(\n",
      "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (i_downsample): Sequential(\n",
      "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "          (1): Block(\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): Block(\n",
      "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (i_downsample): Sequential(\n",
      "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "          (1): Block(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (8): Sequential(\n",
      "          (0): Block(\n",
      "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (batch_norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (i_downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "          (1): Block(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (batch_norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (mmwave_extractor): mmwave_feature_extractor(\n",
      "      (part): Sequential(\n",
      "        (0): Backbone(\n",
      "          (fc1): Sequential(\n",
      "            (0): Linear(in_features=5, out_features=32, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (transformer1): TransformerBlock(\n",
      "            (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "            (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (fc_delta): Sequential(\n",
      "              (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "              (1): ReLU()\n",
      "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (fc_gamma): Sequential(\n",
      "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (1): ReLU()\n",
      "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "            )\n",
      "            (w_qs): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (w_ks): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (w_vs): Linear(in_features=128, out_features=128, bias=False)\n",
      "          )\n",
      "          (transition_downs): ModuleList(\n",
      "            (0): TransitionDown(\n",
      "              (conv1): Sequential(\n",
      "                (0): Conv1d(35, 64, kernel_size=(1,), stride=(1,))\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU()\n",
      "              )\n",
      "              (conv2): Sequential(\n",
      "                (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "                (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU()\n",
      "              )\n",
      "            )\n",
      "            (1): TransitionDown(\n",
      "              (conv1): Sequential(\n",
      "                (0): Conv1d(67, 128, kernel_size=(1,), stride=(1,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU()\n",
      "              )\n",
      "              (conv2): Sequential(\n",
      "                (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "                (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU()\n",
      "              )\n",
      "            )\n",
      "            (2): TransitionDown(\n",
      "              (conv1): Sequential(\n",
      "                (0): Conv1d(131, 256, kernel_size=(1,), stride=(1,))\n",
      "                (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU()\n",
      "              )\n",
      "              (conv2): Sequential(\n",
      "                (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "                (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU()\n",
      "              )\n",
      "            )\n",
      "            (3): TransitionDown(\n",
      "              (conv1): Sequential(\n",
      "                (0): Conv1d(259, 512, kernel_size=(1,), stride=(1,))\n",
      "                (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU()\n",
      "              )\n",
      "              (conv2): Sequential(\n",
      "                (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "                (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (2): ReLU()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (transformers): ModuleList(\n",
      "            (0): TransformerBlock(\n",
      "              (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
      "              (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "              (fc_delta): Sequential(\n",
      "                (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "                (1): ReLU()\n",
      "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "              )\n",
      "              (fc_gamma): Sequential(\n",
      "                (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (1): ReLU()\n",
      "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "              )\n",
      "              (w_qs): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (w_ks): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (w_vs): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (1): TransformerBlock(\n",
      "              (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (fc_delta): Sequential(\n",
      "                (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "                (1): ReLU()\n",
      "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "              )\n",
      "              (fc_gamma): Sequential(\n",
      "                (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (1): ReLU()\n",
      "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "              )\n",
      "              (w_qs): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (w_ks): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (w_vs): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (2): TransformerBlock(\n",
      "              (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
      "              (fc_delta): Sequential(\n",
      "                (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "                (1): ReLU()\n",
      "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "              )\n",
      "              (fc_gamma): Sequential(\n",
      "                (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (1): ReLU()\n",
      "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "              )\n",
      "              (w_qs): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (w_ks): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (w_vs): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "            (3): TransformerBlock(\n",
      "              (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (fc2): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (fc_delta): Sequential(\n",
      "                (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "                (1): ReLU()\n",
      "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "              )\n",
      "              (fc_gamma): Sequential(\n",
      "                (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (1): ReLU()\n",
      "                (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "              )\n",
      "              (w_qs): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (w_ks): Linear(in_features=128, out_features=128, bias=False)\n",
      "              (w_vs): Linear(in_features=128, out_features=128, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lidar_extractor): lidar_feature_extractor(\n",
      "      (fc1): Sequential(\n",
      "        (0): Linear(in_features=3, out_features=32, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (transformer1): TransformerBlock(\n",
      "        (fc1): Linear(in_features=32, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=32, bias=True)\n",
      "        (fc_delta): Sequential(\n",
      "          (0): Linear(in_features=3, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (fc_gamma): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (w_qs): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (w_ks): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (w_vs): Linear(in_features=512, out_features=512, bias=False)\n",
      "      )\n",
      "      (transition_downs): ModuleList(\n",
      "        (0): TransitionDown(\n",
      "          (sa): PointNetSetAbstraction(\n",
      "            (mlp_convs): ModuleList(\n",
      "              (0): Conv2d(35, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (mlp_bns): ModuleList(\n",
      "              (0-1): 2 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (transformers): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
      "          (fc_delta): Sequential(\n",
      "            (0): Linear(in_features=3, out_features=512, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (fc_gamma): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (w_qs): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (w_ks): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (w_vs): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (csi_extractor): csi_feature_extractor(\n",
      "      (part): Sequential(\n",
      "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (4): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (3): BasicBlock(\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (3): BasicBlock(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (4): BasicBlock(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (5): BasicBlock(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (2): BasicBlock(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear_projector): linear_projector(\n",
      "    (linear_projection): Linear(in_features=512, out_features=256, bias=True)\n",
      "  )\n",
      "  (vit): ViT(\n",
      "    (0): TransformerEncoder(\n",
      "      (0): TransformerEncoderBlock(\n",
      "        (0): ResidualAdd(\n",
      "          (fn): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): MultiHeadAttention(\n",
      "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "              (att_drop): Dropout(p=0.0, inplace=False)\n",
      "              (projection): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (2): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): ResidualAdd(\n",
      "          (fn): Sequential(\n",
      "            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): FeedForwardBlock(\n",
      "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            )\n",
      "            (2): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): regression_Head(\n",
      "      (0): Reduce('b n e -> b e', 'mean')\n",
      "      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): Linear(in_features=256, out_features=51, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
