{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from torch import nn\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import yaml\n",
    "from evaluate import error\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "from torchvision.transforms import Resize\n",
    "from syn_DI_dataset import make_dataset, make_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ALL Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rgb_feature_extractor(nn.Module):\n",
    "    def __init__(self, rgb_model):\n",
    "        super(rgb_feature_extractor, self).__init__()\n",
    "        self.part = nn.Sequential(*list(rgb_model.children())[:-2])\n",
    "    def forward(self, x):\n",
    "        x = self.part(x).view(x.size(0), 512, -1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rgb_feature_extractor(\n",
       "  (part): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 3, kernel_size=(14, 14), stride=(2, 2))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(3, 3, kernel_size=(5, 56), stride=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(3, 3, kernel_size=(5, 23), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(3, 16, kernel_size=(3, 14), stride=(1, 1))\n",
       "    )\n",
       "    (1): Conv2d(16, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (5): Sequential(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (i_downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (i_downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (batch_norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (i_downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from RGB_benchmark.rgb_ResNet18.RGB_ResNet import *\n",
    "\n",
    "rgb_model = RGB_ResNet18()\n",
    "rgb_model.load_state_dict(torch.load('./RGB_benchmark/rgb_ResNet18/RGB_Resnet18_copy.pt'))\n",
    "rgb_extractor = rgb_feature_extractor(rgb_model)\n",
    "rgb_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 49, 512])\n"
     ]
    }
   ],
   "source": [
    "test = torch.rand(1, 3, 480, 640)\n",
    "out = rgb_extractor(test)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class depth_feature_extractor(nn.Module):\n",
    "    def __init__(self, depth_model):\n",
    "        super(depth_feature_extractor, self).__init__()\n",
    "        self.part = nn.Sequential(*list(depth_model.children())[:-2])\n",
    "    def forward(self, x):\n",
    "        x = self.part(x).view(x.size(0), 512, -1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "depth_feature_extractor(\n",
       "  (part): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 3, kernel_size=(14, 14), stride=(2, 2))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(3, 3, kernel_size=(5, 56), stride=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(3, 3, kernel_size=(5, 23), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(3, 16, kernel_size=(3, 14), stride=(1, 1))\n",
       "    )\n",
       "    (1): Conv2d(16, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (5): Sequential(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (i_downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (i_downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Block(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (batch_norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (i_downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batch_norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from depth_benchmark.depth_ResNet18 import *\n",
    "depth_model = Depth_ResNet18()\n",
    "depth_model.load_state_dict(torch.load('depth_benchmark/depth_Resnet18.pt'))\n",
    "depth_extractor = depth_feature_extractor(depth_model)\n",
    "depth_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 49, 512])\n"
     ]
    }
   ],
   "source": [
    "test = torch.rand(1, 3, 480, 640)\n",
    "out = depth_extractor(test)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mmWave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mmwave_feature_extractor(nn.Module):\n",
    "    def __init__(self, mmwave_model):\n",
    "        super(mmwave_feature_extractor, self).__init__()\n",
    "        self.part = nn.Sequential(*list(mmwave_model.children())[:-1])\n",
    "    def forward(self, x):\n",
    "        x, _ = self.part(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mmwave_feature_extractor(\n",
       "  (part): Sequential(\n",
       "    (0): Backbone(\n",
       "      (fc1): Sequential(\n",
       "        (0): Linear(in_features=5, out_features=32, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (transformer1): TransformerBlock(\n",
       "        (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (fc_delta): Sequential(\n",
       "          (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (fc_gamma): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (w_qs): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (w_ks): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (w_vs): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (transition_downs): ModuleList(\n",
       "        (0): TransitionDown(\n",
       "          (conv1): Sequential(\n",
       "            (0): Conv1d(35, 64, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (1): TransitionDown(\n",
       "          (conv1): Sequential(\n",
       "            (0): Conv1d(67, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (2): TransitionDown(\n",
       "          (conv1): Sequential(\n",
       "            (0): Conv1d(131, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (3): TransitionDown(\n",
       "          (conv1): Sequential(\n",
       "            (0): Conv1d(259, 512, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (transformers): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (fc_delta): Sequential(\n",
       "            (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (fc_gamma): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (w_qs): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (w_ks): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (w_vs): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_delta): Sequential(\n",
       "            (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (fc_gamma): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (w_qs): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (w_ks): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (w_vs): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (fc_delta): Sequential(\n",
       "            (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (fc_gamma): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (w_qs): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (w_ks): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (w_vs): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (fc2): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (fc_delta): Sequential(\n",
       "            (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (fc_gamma): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (w_qs): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (w_ks): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (w_vs): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mmwave_benchmark.mmwave_point_transformer import *\n",
    "mmwave_model = mmwave_PointTransformerReg()\n",
    "mmwave_model.load_state_dict(torch.load('mmwave_benchmark/mmwave_all_random.pt'))\n",
    "mmwave_extractor = mmwave_feature_extractor(mmwave_model)\n",
    "mmwave_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 57, 512])\n"
     ]
    }
   ],
   "source": [
    "test = torch.rand(1, 57, 5)\n",
    "out = mmwave_extractor(test)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lidar_feature_extractor(nn.Module):\n",
    "    def __init__(self, lidar_model):\n",
    "        super(lidar_feature_extractor, self).__init__()\n",
    "        # self.model = lidar_model\n",
    "        npoints, nblocks, nneighbor, n_c, d_points = 1024, 5, 16, 51, 3\n",
    "        self.fc1 = lidar_model.backbone.fc1\n",
    "        self.transformer1 = lidar_model.backbone.transformer1\n",
    "        self.transition_downs = nn.ModuleList()\n",
    "        self.transformers = nn.ModuleList()\n",
    "        for i in range(nblocks - 4):\n",
    "            channel = 32 * 2 ** (i + 1)\n",
    "            self.transition_downs.append(lidar_model.backbone.transition_downs[i])\n",
    "            self.transformers.append(lidar_model.backbone.transformers[i])\n",
    "        self.nblocks = nblocks\n",
    "    \n",
    "    def forward(self, x):\n",
    "        xyz = x[..., :3]\n",
    "        points = self.transformer1(xyz, self.fc1(x))[0]\n",
    "\n",
    "        xyz_and_feats = [(xyz, points)]\n",
    "        for i in range(self.nblocks - 4):\n",
    "            xyz, points = self.transition_downs[i](xyz, points)\n",
    "            points = self.transformers[i](xyz, points)[0]\n",
    "            xyz_and_feats.append((xyz, points))\n",
    "        points = points.view(points.size(0), -1, 512)\n",
    "        return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lidar_feature_extractor(\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  )\n",
       "  (transformer1): TransformerBlock(\n",
       "    (fc1): Linear(in_features=32, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=32, bias=True)\n",
       "    (fc_delta): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (fc_gamma): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (w_qs): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (w_ks): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (w_vs): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (transition_downs): ModuleList(\n",
       "    (0): TransitionDown(\n",
       "      (sa): PointNetSetAbstraction(\n",
       "        (mlp_convs): ModuleList(\n",
       "          (0): Conv2d(35, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (mlp_bns): ModuleList(\n",
       "          (0-1): 2 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformers): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
       "      (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
       "      (fc_delta): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (fc_gamma): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (w_qs): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (w_ks): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (w_vs): Linear(in_features=512, out_features=512, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lidar_benchmark.lidar_point_transformer import *\n",
    "lidar_model = PointTransformerReg()\n",
    "lidar_model.load_state_dict(torch.load('lidar_benchmark/lidar_all_random.pt'))\n",
    "lidar_extractor = lidar_feature_extractor(lidar_model)\n",
    "lidar_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "test = torch.rand(1, 365, 3)\n",
    "out = lidar_extractor(test)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WiFI-CSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class csi_feature_extractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(csi_feature_extractor, self).__init__()\n",
    "        self.part = nn.Sequential(\n",
    "            model.encoder_conv1,\n",
    "            model.encoder_bn1,\n",
    "            model.encoder_relu,\n",
    "            model.encoder_layer1,\n",
    "            model.encoder_layer2,\n",
    "            model.encoder_layer3,\n",
    "            model.encoder_layer4, \n",
    "            # torch.nn.AvgPool2d((1, 4))\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = torch.transpose(x, 2, 3) #16,2,114,3,32\n",
    "        x = torch.flatten(x, 3, 4)# 16,2,114,96\n",
    "        torch_resize = Resize([136,32])\n",
    "        x = torch_resize(x)\n",
    "        x = self.part(x).view(x.size(0), 512, -1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csi_feature_extractor(\n",
       "  (part): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './CSI_benchmark')\n",
    "csi_model = torch.load('CSI_benchmark/protocol3_random_1.pkl')\n",
    "csi_extractor = csi_feature_extractor(csi_model)\n",
    "csi_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 68, 512])\n"
     ]
    }
   ],
   "source": [
    "test = torch.rand(32, 3, 114, 10).cuda()\n",
    "out = csi_extractor(test) \n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_extractor = rgb_extractor.cuda()\n",
    "# input shape: B, 3, 480, 640; output shape: B, 49, 512\n",
    "depth_extractor = depth_extractor.cuda()\n",
    "# input shape: B, 3, 480, 640; output shape: B, 49, 512\n",
    "mmwave_extractor = mmwave_extractor.cuda()\n",
    "# input shape: B, n, 5; output shape: 1, n, 512\n",
    "lidar_extractor = lidar_extractor.cuda()\n",
    "# input shape: B, n, 3; output shape: B, 32, 512\n",
    "csi_extractor = csi_extractor.cuda()\n",
    "# input shape: B, 3, 114, 10; output shape: B, 68, 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_extrator(nn.Module):\n",
    "    def __init__(self, rgb_extractor, depth_extractor, mmwave_extractor, lidar_extractor, csi_extractor):\n",
    "        super(feature_extrator, self).__init__()\n",
    "        self.rgb_extractor = rgb_extractor\n",
    "        self.depth_extractor = depth_extractor\n",
    "        self.mmwave_extractor = mmwave_extractor\n",
    "        self.lidar_extractor = lidar_extractor\n",
    "        self.csi_extractor = csi_extractor\n",
    "    def forward(self, rgb_data, depth_data, mmwave_data, lidar_data, csi_data):\n",
    "        rgb_feature = self.rgb_extractor(rgb_data)\n",
    "        depth_feature = self.depth_extractor(depth_data)\n",
    "        mmwave_feature = self.mmwave_extractor(mmwave_data)\n",
    "        lidar_feature = self.lidar_extractor(lidar_data)\n",
    "        csi_feature = self.csi_extractor(csi_data)\n",
    "        return rgb_feature, depth_feature, mmwave_feature, lidar_feature, csi_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear projection on seq len dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_projection(nn.Module):\n",
    "    def __init__(self, rgb_dim, depth_dim, mmwave_dim, lidar_dim, csi_dim, output_dim):\n",
    "        super(linear_projection, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.rgb_linear = nn.Linear(rgb_dim, output_dim)\n",
    "        self.depth_linear = nn.Linear(depth_dim, output_dim)\n",
    "        self.mmwave_linear = nn.Linear(mmwave_dim, output_dim)\n",
    "        self.lidar_linear = nn.Linear(lidar_dim, output_dim)\n",
    "        self.csi_linear = nn.Linear(csi_dim, output_dim)\n",
    "    def forward(self, rgb_feature, depth_feature, mmwave_feature, lidar_feature, csi_feature, modality_list):\n",
    "        rgb_feature = self.rgb_linear(rgb_feature.permute(0, 2, 1))\n",
    "        depth_feature = self.depth_linear(depth_feature.permute(0, 2, 1))\n",
    "        mmwave_feature = self.mmwave_linear(mmwave_feature.permute(0, 2, 1))\n",
    "        lidar_feature = self.lidar_linear(lidar_feature.permute(0, 2, 1))\n",
    "        csi_feature = self.csi_linear(csi_feature.permute(0, 2, 1))\n",
    "        features = torch.cat([rgb_feature, depth_feature, mmwave_feature, lidar_feature, csi_feature], dim=2)\n",
    "        empty_feature = torch.zeros_like(rgb_feature, device=torch.device('cuda'))\n",
    "        for i in range(len(modality_list)):\n",
    "            if modality_list[i] == True:\n",
    "                continue\n",
    "            else:\n",
    "                features[:, :, i*self.output_dim:(i+1)*self.output_dim] = empty_feature\n",
    "        features = features.permute(0, 2, 1)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear projection on patch embedding dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_projection(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(linear_projection, self).__init__()\n",
    "        self.linear_projection = nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, rgb_feature, depth_feature, mmwave_feature, lidar_feature, csi_feature, modality_list):\n",
    "        feature_list = [rgb_feature, depth_feature, mmwave_feature, lidar_feature, csi_feature]\n",
    "        if len (modality_list) == 0:\n",
    "            print('WARNING: modality_list is empty!')\n",
    "            feature = torch.zeros_like(lidar_feature, device=torch.device('cuda'))\n",
    "            feature = self.linear_projection(feature)\n",
    "        else:\n",
    "            real_feature_list = []\n",
    "            for i in range(len(modality_list)):\n",
    "                if modality_list[i] == True:\n",
    "                    real_feature_list.append(feature_list[i])\n",
    "                else:\n",
    "                    continue\n",
    "            feature = torch.cat(real_feature_list, dim=1)\n",
    "            feature = self.linear_projection(feature)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 261, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rgb_data = torch.rand(16, 3, 480, 640).cuda()\n",
    "depth_data = torch.rand(16, 3, 480, 640).cuda()\n",
    "mmwave_data = torch.rand(16, 63, 5).cuda()\n",
    "lidar_data = torch.rand(16, 1467, 3).cuda()\n",
    "csi_data = torch.rand(16, 3, 114, 10).cuda()\n",
    "\n",
    "feature_extractor = feature_extrator(rgb_extractor, depth_extractor, mmwave_extractor, lidar_extractor, csi_extractor).cuda()\n",
    "rgb_feature, depth_feature, mmwave_feature, lidar_feature, csi_feature = feature_extractor(rgb_data, depth_data, mmwave_data, lidar_data, csi_data)\n",
    "\n",
    "modality_list = [True, True, True, True, True]\n",
    "\n",
    "linear_projector = linear_projection(512, 128).cuda()\n",
    "feature = linear_projector(rgb_feature, depth_feature, mmwave_feature, lidar_feature, csi_feature, modality_list)\n",
    "print(feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size = 225, num_heads = 4, dropout = 0.0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = nn.Linear(emb_size, emb_size*3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "        \n",
    "        scaling = self.emb_size ** (1/2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion = 4, drop_p = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "        \n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size = 225,\n",
    "                 drop_p = 0.5,\n",
    "                 forward_expansion = 4,\n",
    "                 forward_drop_p = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "        \n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth = 1, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "        \n",
    "class regression_Head(nn.Sequential):\n",
    "    def __init__(self, emb_size, num_classes):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, num_classes))\n",
    "        \n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                emb_size = 128,\n",
    "                depth = 1,\n",
    "                *,\n",
    "                num_classes = 17*3,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            regression_Head(emb_size, num_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 51])\n"
     ]
    }
   ],
   "source": [
    "vit = ViT().cuda()\n",
    "out = vit(feature)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modality_invariant_model(nn.Module):\n",
    "    def __init__(self, feature_extractor, linear_projector, vit):\n",
    "        super(modality_invariant_model, self).__init__()\n",
    "        self.feature_extractor = feature_extrator(rgb_extractor, depth_extractor, mmwave_extractor, lidar_extractor, csi_extractor)\n",
    "        self.linear_projector = linear_projector\n",
    "        self.vit = vit\n",
    "    def forward(self, rgb_data, depth_data, mmwave_data, lidar_data, csi_data, modality_list):\n",
    "        rgb_feature, depth_feature, mmwave_feature, lidar_feature, csi_feature = self.feature_extractor(rgb_data, depth_data, mmwave_data, lidar_data, csi_data)\n",
    "        feature = self.linear_projector(rgb_feature, depth_feature, mmwave_feature, lidar_feature, csi_feature, modality_list)\n",
    "        out = self.vit(feature)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.43 GiB. GPU 0 has a total capacty of 23.99 GiB of which 0 bytes is free. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 360.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CHEN_X~1\\AppData\\Local\\Temp/ipykernel_42180/1378958069.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodality_invariant_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinear_projector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmwave_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlidar_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsi_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodality_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\CHEN_X~1\\AppData\\Local\\Temp/ipykernel_42180/191186454.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, rgb_data, depth_data, mmwave_data, lidar_data, csi_data, modality_list)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrgb_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmwave_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlidar_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsi_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodality_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mrgb_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmwave_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlidar_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsi_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmwave_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlidar_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsi_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mfeature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_projector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmwave_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlidar_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsi_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodality_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\CHEN_X~1\\AppData\\Local\\Temp/ipykernel_42180/594496936.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, rgb_data, depth_data, mmwave_data, lidar_data, csi_data)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mdepth_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepth_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mmmwave_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmmwave_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmmwave_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mlidar_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlidar_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlidar_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mcsi_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsi_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsi_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrgb_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmwave_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlidar_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsi_feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\CHEN_X~1\\AppData\\Local\\Temp/ipykernel_42180/1727733702.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mxyz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mpoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxyz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mxyz_and_feats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxyz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chen_Xinyan\\Desktop\\Modality_Invariant\\lidar_benchmark\\lidar_point_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, xyz, features)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mpos_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_delta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxyz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mknn_xyz\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# b x n x k x f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_gamma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpos_enc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# b x n x k x f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chen_Xinyan\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1469\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1470\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1471\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1472\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.43 GiB. GPU 0 has a total capacty of 23.99 GiB of which 0 bytes is free. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 360.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "rgb_data = torch.rand(16, 3, 480, 640).cuda()\n",
    "depth_data = torch.rand(16, 3, 480, 640).cuda()\n",
    "mmwave_data = torch.rand(16, 63, 5).cuda()\n",
    "lidar_data = torch.rand(16, 1467, 3).cuda()\n",
    "csi_data = torch.rand(16, 3, 114, 10).cuda()\n",
    "modality_list = random.choices(\n",
    "    [True, False],\n",
    "    k= 5,\n",
    "    weights=[80, 20]\n",
    ")\n",
    "model = modality_invariant_model(feature_extractor, linear_projector, vit).cuda()\n",
    "out = model(rgb_data, depth_data, mmwave_data, lidar_data, csi_data, modality_list)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, False, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "result = random.choices(\n",
    "    [True, False],\n",
    "    k= 5,\n",
    "    weights=[80, 20]\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check parameters prove to be the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.1442, -0.0400,  0.0629],\n",
      "          [-0.1392,  0.0949,  0.2094],\n",
      "          [ 0.0652, -0.2481, -0.3184]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0702, -0.2592, -0.1461],\n",
      "          [ 0.0290, -0.2670, -0.1601],\n",
      "          [-0.0984, -0.1065,  0.1317]]],\n",
      "\n",
      "\n",
      "        [[[-0.2569, -0.1761, -0.2460],\n",
      "          [-0.0025,  0.0818, -0.1916],\n",
      "          [ 0.0658,  0.0344, -0.1465]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1116, -0.2394, -0.2183],\n",
      "          [-0.0552, -0.1350, -0.1518],\n",
      "          [ 0.0485, -0.0301, -0.2722]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1532, -0.1014, -0.0618],\n",
      "          [ 0.3229, -0.0714,  0.1710],\n",
      "          [-0.1570,  0.2686,  0.3306]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1332, -0.3275, -0.0421],\n",
      "          [-0.2583, -0.3117, -0.2423],\n",
      "          [-0.1054,  0.2280,  0.0535]]],\n",
      "\n",
      "\n",
      "        [[[-0.1630, -0.1043,  0.2208],\n",
      "          [ 0.1549, -0.2130,  0.0888],\n",
      "          [-0.1122, -0.2864,  0.3296]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2309, -0.1619, -0.2679],\n",
      "          [-0.2516,  0.0649,  0.2708],\n",
      "          [ 0.0679, -0.0229, -0.0502]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0256, -0.2149, -0.3349],\n",
      "          [ 0.0192, -0.2953,  0.2668],\n",
      "          [-0.1498, -0.1509, -0.3249]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1865,  0.2855,  0.1936],\n",
      "          [ 0.2351, -0.2628, -0.3159],\n",
      "          [-0.1037,  0.0915, -0.0338]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1967, -0.2587,  0.2417],\n",
      "          [-0.0830,  0.2266, -0.0038],\n",
      "          [ 0.2460,  0.2683, -0.2962]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0339, -0.1544, -0.2188],\n",
      "          [ 0.0099, -0.3039,  0.2128],\n",
      "          [-0.0962, -0.2907,  0.0627]]],\n",
      "\n",
      "\n",
      "        [[[-0.3378, -0.1602, -0.1381],\n",
      "          [ 0.1908, -0.0842, -0.3077],\n",
      "          [-0.2165, -0.0832,  0.2656]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3188, -0.2482, -0.2300],\n",
      "          [-0.0125, -0.2635, -0.0508],\n",
      "          [ 0.2027,  0.2090,  0.2053]]],\n",
      "\n",
      "\n",
      "        [[[-0.3059, -0.0226, -0.3294],\n",
      "          [-0.0068,  0.1072, -0.1756],\n",
      "          [-0.0273, -0.1036, -0.1347]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0519,  0.0058, -0.2551],\n",
      "          [ 0.0993, -0.0333, -0.1382],\n",
      "          [-0.1868,  0.3116, -0.0939]]],\n",
      "\n",
      "\n",
      "        [[[-0.0865,  0.2580, -0.2693],\n",
      "          [-0.0029,  0.2312, -0.3279],\n",
      "          [-0.2430,  0.0167,  0.2314]]],\n",
      "\n",
      "\n",
      "        [[[-0.0255, -0.0329,  0.3047],\n",
      "          [-0.3021,  0.0302,  0.0549],\n",
      "          [-0.0105,  0.3301,  0.1948]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2782,  0.2772,  0.1922],\n",
      "          [-0.2292, -0.2305,  0.0071],\n",
      "          [-0.0338, -0.0273, -0.1231]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3062, -0.1779,  0.2758],\n",
      "          [-0.0972,  0.2229,  0.0539],\n",
      "          [-0.2263,  0.0776, -0.0488]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2654, -0.3255,  0.0028],\n",
      "          [-0.2280, -0.2733, -0.2864],\n",
      "          [ 0.2095,  0.2218, -0.1040]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1979, -0.1147,  0.0509],\n",
      "          [ 0.2827, -0.1322, -0.2046],\n",
      "          [ 0.2085,  0.3049,  0.1304]]],\n",
      "\n",
      "\n",
      "        [[[-0.1373, -0.0609, -0.0981],\n",
      "          [ 0.2052,  0.1276,  0.1632],\n",
      "          [-0.0788,  0.1634, -0.2274]]],\n",
      "\n",
      "\n",
      "        [[[-0.0899,  0.1097,  0.1124],\n",
      "          [-0.2838, -0.0452, -0.0562],\n",
      "          [ 0.1448, -0.2134,  0.2742]]],\n",
      "\n",
      "\n",
      "        [[[-0.3358,  0.1567, -0.3250],\n",
      "          [-0.0022,  0.0309,  0.3089],\n",
      "          [-0.0393,  0.1873, -0.3134]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1116, -0.1143, -0.1365],\n",
      "          [ 0.2278,  0.3096, -0.3200],\n",
      "          [ 0.2548,  0.2990, -0.0597]]],\n",
      "\n",
      "\n",
      "        [[[-0.1638, -0.0117,  0.2373],\n",
      "          [-0.1387,  0.0521, -0.1817],\n",
      "          [-0.0507, -0.1589,  0.3000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2603,  0.1751, -0.0896],\n",
      "          [ 0.1284, -0.2312,  0.0344],\n",
      "          [ 0.2093,  0.1570, -0.1849]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0541, -0.0332,  0.2752],\n",
      "          [-0.2909, -0.1505,  0.1537],\n",
      "          [ 0.1180, -0.1458, -0.1900]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1280,  0.0912,  0.1253],\n",
      "          [-0.1342,  0.2969,  0.3306],\n",
      "          [-0.2393,  0.1278,  0.0613]]],\n",
      "\n",
      "\n",
      "        [[[-0.2865,  0.2456,  0.2278],\n",
      "          [-0.2297,  0.0107, -0.0528],\n",
      "          [-0.2689, -0.1288, -0.1287]]],\n",
      "\n",
      "\n",
      "        [[[-0.0762,  0.1788,  0.3005],\n",
      "          [-0.2271,  0.0972, -0.0801],\n",
      "          [-0.2075, -0.1848, -0.1615]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2261,  0.2184,  0.0116],\n",
      "          [-0.0666,  0.2969,  0.1704],\n",
      "          [ 0.1084,  0.2028,  0.0391]]],\n",
      "\n",
      "\n",
      "        [[[-0.0506, -0.1576, -0.2370],\n",
      "          [-0.3139, -0.1425,  0.1543],\n",
      "          [-0.1583,  0.2508, -0.0099]]],\n",
      "\n",
      "\n",
      "        [[[-0.0982,  0.1321, -0.1768],\n",
      "          [ 0.2928,  0.2512, -0.0667],\n",
      "          [-0.1631, -0.1654,  0.1208]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0566, -0.1968, -0.1713],\n",
      "          [ 0.1281, -0.1888, -0.0948],\n",
      "          [-0.3331, -0.2695,  0.3211]]],\n",
      "\n",
      "\n",
      "        [[[-0.0777,  0.0592, -0.0174],\n",
      "          [ 0.0604,  0.0510,  0.1830],\n",
      "          [-0.2966, -0.0227, -0.1359]]],\n",
      "\n",
      "\n",
      "        [[[-0.1982,  0.0051, -0.0662],\n",
      "          [-0.1411, -0.2276, -0.0564],\n",
      "          [-0.0407,  0.1526, -0.1299]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3075, -0.0637,  0.2596],\n",
      "          [-0.1934,  0.2197,  0.0129],\n",
      "          [-0.3089, -0.1520, -0.1126]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2478, -0.2864,  0.1405],\n",
      "          [ 0.3278, -0.0692,  0.0287],\n",
      "          [ 0.2049, -0.0891, -0.3022]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3060,  0.2521,  0.0820],\n",
      "          [ 0.1908, -0.2901,  0.2090],\n",
      "          [-0.2438, -0.0005, -0.2065]]],\n",
      "\n",
      "\n",
      "        [[[-0.2115,  0.1457, -0.1816],\n",
      "          [-0.2924,  0.3199, -0.0570],\n",
      "          [-0.1252,  0.0150, -0.1796]]],\n",
      "\n",
      "\n",
      "        [[[-0.0733,  0.2871, -0.1806],\n",
      "          [-0.0710, -0.0960,  0.0983],\n",
      "          [-0.2175, -0.1457,  0.1222]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1849, -0.2970, -0.3010],\n",
      "          [ 0.1445,  0.2162, -0.1427],\n",
      "          [ 0.0855,  0.0561,  0.1802]]],\n",
      "\n",
      "\n",
      "        [[[-0.2003,  0.3153, -0.2129],\n",
      "          [ 0.0997,  0.2821, -0.0877],\n",
      "          [-0.0367, -0.2310, -0.0435]]],\n",
      "\n",
      "\n",
      "        [[[-0.1330,  0.2927, -0.0996],\n",
      "          [ 0.1115, -0.0617,  0.2026],\n",
      "          [ 0.0854,  0.1284, -0.3189]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0394, -0.0737,  0.2335],\n",
      "          [ 0.0315,  0.1024,  0.0191],\n",
      "          [ 0.1007,  0.2212, -0.0246]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0097, -0.3317,  0.0495],\n",
      "          [ 0.0605,  0.2045,  0.1617],\n",
      "          [-0.2588,  0.2382,  0.1020]]],\n",
      "\n",
      "\n",
      "        [[[-0.2911,  0.2101, -0.0863],\n",
      "          [-0.1851,  0.2725,  0.1455],\n",
      "          [-0.0712,  0.0146, -0.1010]]],\n",
      "\n",
      "\n",
      "        [[[-0.0280, -0.1059,  0.1672],\n",
      "          [ 0.0161, -0.0418, -0.0293],\n",
      "          [-0.2915,  0.3177,  0.1607]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1499,  0.0744, -0.0609],\n",
      "          [ 0.2741,  0.2634, -0.3023],\n",
      "          [ 0.2327, -0.2497, -0.1559]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1928,  0.2180,  0.0317],\n",
      "          [-0.1215,  0.2550, -0.1272],\n",
      "          [-0.1832,  0.1956,  0.1641]]],\n",
      "\n",
      "\n",
      "        [[[-0.0098, -0.1728, -0.1748],\n",
      "          [ 0.3310, -0.2141,  0.1293],\n",
      "          [-0.0609,  0.2041, -0.0072]]],\n",
      "\n",
      "\n",
      "        [[[-0.2827, -0.1463, -0.2361],\n",
      "          [-0.0720,  0.1963, -0.3078],\n",
      "          [ 0.1393, -0.0894,  0.2399]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1822, -0.1961,  0.3163],\n",
      "          [ 0.1461,  0.2640, -0.2712],\n",
      "          [-0.2681, -0.1732, -0.2507]]],\n",
      "\n",
      "\n",
      "        [[[-0.3288,  0.1626, -0.3206],\n",
      "          [ 0.0645, -0.2031,  0.0248],\n",
      "          [ 0.1724,  0.1508, -0.0245]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0473,  0.1761,  0.2205],\n",
      "          [ 0.1047, -0.2445, -0.3073],\n",
      "          [ 0.1859,  0.2633, -0.1064]]],\n",
      "\n",
      "\n",
      "        [[[-0.2603,  0.1408,  0.1599],\n",
      "          [ 0.0552,  0.1244,  0.0027],\n",
      "          [-0.0082, -0.0621, -0.2353]]],\n",
      "\n",
      "\n",
      "        [[[-0.1342, -0.2685,  0.2826],\n",
      "          [ 0.3174, -0.1614, -0.0045],\n",
      "          [-0.0831, -0.0713, -0.2345]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0136,  0.1717, -0.2375],\n",
      "          [ 0.3174,  0.3134,  0.0546],\n",
      "          [-0.1241, -0.0263, -0.0861]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2216,  0.2891, -0.0102],\n",
      "          [-0.0579,  0.1079,  0.0554],\n",
      "          [ 0.2561,  0.0748, -0.0820]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0668,  0.1625,  0.1169],\n",
      "          [ 0.1584,  0.2911,  0.1805],\n",
      "          [ 0.0081,  0.3097, -0.1904]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1527,  0.2572, -0.2060],\n",
      "          [ 0.3115, -0.3122, -0.1887],\n",
      "          [-0.0953, -0.0323, -0.1646]]],\n",
      "\n",
      "\n",
      "        [[[-0.0800, -0.3182, -0.2164],\n",
      "          [-0.2183,  0.2814, -0.0991],\n",
      "          [-0.0655, -0.3143, -0.3171]]]], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CHEN_X~1\\AppData\\Local\\Temp/ipykernel_13504/1360833873.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsi_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "for param in csi_model.parameters():\n",
    "    print(param)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.1442, -0.0400,  0.0629],\n",
      "          [-0.1392,  0.0949,  0.2094],\n",
      "          [ 0.0652, -0.2481, -0.3184]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0702, -0.2592, -0.1461],\n",
      "          [ 0.0290, -0.2670, -0.1601],\n",
      "          [-0.0984, -0.1065,  0.1317]]],\n",
      "\n",
      "\n",
      "        [[[-0.2569, -0.1761, -0.2460],\n",
      "          [-0.0025,  0.0818, -0.1916],\n",
      "          [ 0.0658,  0.0344, -0.1465]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1116, -0.2394, -0.2183],\n",
      "          [-0.0552, -0.1350, -0.1518],\n",
      "          [ 0.0485, -0.0301, -0.2722]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1532, -0.1014, -0.0618],\n",
      "          [ 0.3229, -0.0714,  0.1710],\n",
      "          [-0.1570,  0.2686,  0.3306]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1332, -0.3275, -0.0421],\n",
      "          [-0.2583, -0.3117, -0.2423],\n",
      "          [-0.1054,  0.2280,  0.0535]]],\n",
      "\n",
      "\n",
      "        [[[-0.1630, -0.1043,  0.2208],\n",
      "          [ 0.1549, -0.2130,  0.0888],\n",
      "          [-0.1122, -0.2864,  0.3296]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2309, -0.1619, -0.2679],\n",
      "          [-0.2516,  0.0649,  0.2708],\n",
      "          [ 0.0679, -0.0229, -0.0502]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0256, -0.2149, -0.3349],\n",
      "          [ 0.0192, -0.2953,  0.2668],\n",
      "          [-0.1498, -0.1509, -0.3249]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1865,  0.2855,  0.1936],\n",
      "          [ 0.2351, -0.2628, -0.3159],\n",
      "          [-0.1037,  0.0915, -0.0338]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1967, -0.2587,  0.2417],\n",
      "          [-0.0830,  0.2266, -0.0038],\n",
      "          [ 0.2460,  0.2683, -0.2962]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0339, -0.1544, -0.2188],\n",
      "          [ 0.0099, -0.3039,  0.2128],\n",
      "          [-0.0962, -0.2907,  0.0627]]],\n",
      "\n",
      "\n",
      "        [[[-0.3378, -0.1602, -0.1381],\n",
      "          [ 0.1908, -0.0842, -0.3077],\n",
      "          [-0.2165, -0.0832,  0.2656]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3188, -0.2482, -0.2300],\n",
      "          [-0.0125, -0.2635, -0.0508],\n",
      "          [ 0.2027,  0.2090,  0.2053]]],\n",
      "\n",
      "\n",
      "        [[[-0.3059, -0.0226, -0.3294],\n",
      "          [-0.0068,  0.1072, -0.1756],\n",
      "          [-0.0273, -0.1036, -0.1347]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0519,  0.0058, -0.2551],\n",
      "          [ 0.0993, -0.0333, -0.1382],\n",
      "          [-0.1868,  0.3116, -0.0939]]],\n",
      "\n",
      "\n",
      "        [[[-0.0865,  0.2580, -0.2693],\n",
      "          [-0.0029,  0.2312, -0.3279],\n",
      "          [-0.2430,  0.0167,  0.2314]]],\n",
      "\n",
      "\n",
      "        [[[-0.0255, -0.0329,  0.3047],\n",
      "          [-0.3021,  0.0302,  0.0549],\n",
      "          [-0.0105,  0.3301,  0.1948]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2782,  0.2772,  0.1922],\n",
      "          [-0.2292, -0.2305,  0.0071],\n",
      "          [-0.0338, -0.0273, -0.1231]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3062, -0.1779,  0.2758],\n",
      "          [-0.0972,  0.2229,  0.0539],\n",
      "          [-0.2263,  0.0776, -0.0488]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2654, -0.3255,  0.0028],\n",
      "          [-0.2280, -0.2733, -0.2864],\n",
      "          [ 0.2095,  0.2218, -0.1040]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1979, -0.1147,  0.0509],\n",
      "          [ 0.2827, -0.1322, -0.2046],\n",
      "          [ 0.2085,  0.3049,  0.1304]]],\n",
      "\n",
      "\n",
      "        [[[-0.1373, -0.0609, -0.0981],\n",
      "          [ 0.2052,  0.1276,  0.1632],\n",
      "          [-0.0788,  0.1634, -0.2274]]],\n",
      "\n",
      "\n",
      "        [[[-0.0899,  0.1097,  0.1124],\n",
      "          [-0.2838, -0.0452, -0.0562],\n",
      "          [ 0.1448, -0.2134,  0.2742]]],\n",
      "\n",
      "\n",
      "        [[[-0.3358,  0.1567, -0.3250],\n",
      "          [-0.0022,  0.0309,  0.3089],\n",
      "          [-0.0393,  0.1873, -0.3134]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1116, -0.1143, -0.1365],\n",
      "          [ 0.2278,  0.3096, -0.3200],\n",
      "          [ 0.2548,  0.2990, -0.0597]]],\n",
      "\n",
      "\n",
      "        [[[-0.1638, -0.0117,  0.2373],\n",
      "          [-0.1387,  0.0521, -0.1817],\n",
      "          [-0.0507, -0.1589,  0.3000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2603,  0.1751, -0.0896],\n",
      "          [ 0.1284, -0.2312,  0.0344],\n",
      "          [ 0.2093,  0.1570, -0.1849]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0541, -0.0332,  0.2752],\n",
      "          [-0.2909, -0.1505,  0.1537],\n",
      "          [ 0.1180, -0.1458, -0.1900]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1280,  0.0912,  0.1253],\n",
      "          [-0.1342,  0.2969,  0.3306],\n",
      "          [-0.2393,  0.1278,  0.0613]]],\n",
      "\n",
      "\n",
      "        [[[-0.2865,  0.2456,  0.2278],\n",
      "          [-0.2297,  0.0107, -0.0528],\n",
      "          [-0.2689, -0.1288, -0.1287]]],\n",
      "\n",
      "\n",
      "        [[[-0.0762,  0.1788,  0.3005],\n",
      "          [-0.2271,  0.0972, -0.0801],\n",
      "          [-0.2075, -0.1848, -0.1615]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2261,  0.2184,  0.0116],\n",
      "          [-0.0666,  0.2969,  0.1704],\n",
      "          [ 0.1084,  0.2028,  0.0391]]],\n",
      "\n",
      "\n",
      "        [[[-0.0506, -0.1576, -0.2370],\n",
      "          [-0.3139, -0.1425,  0.1543],\n",
      "          [-0.1583,  0.2508, -0.0099]]],\n",
      "\n",
      "\n",
      "        [[[-0.0982,  0.1321, -0.1768],\n",
      "          [ 0.2928,  0.2512, -0.0667],\n",
      "          [-0.1631, -0.1654,  0.1208]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0566, -0.1968, -0.1713],\n",
      "          [ 0.1281, -0.1888, -0.0948],\n",
      "          [-0.3331, -0.2695,  0.3211]]],\n",
      "\n",
      "\n",
      "        [[[-0.0777,  0.0592, -0.0174],\n",
      "          [ 0.0604,  0.0510,  0.1830],\n",
      "          [-0.2966, -0.0227, -0.1359]]],\n",
      "\n",
      "\n",
      "        [[[-0.1982,  0.0051, -0.0662],\n",
      "          [-0.1411, -0.2276, -0.0564],\n",
      "          [-0.0407,  0.1526, -0.1299]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3075, -0.0637,  0.2596],\n",
      "          [-0.1934,  0.2197,  0.0129],\n",
      "          [-0.3089, -0.1520, -0.1126]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2478, -0.2864,  0.1405],\n",
      "          [ 0.3278, -0.0692,  0.0287],\n",
      "          [ 0.2049, -0.0891, -0.3022]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3060,  0.2521,  0.0820],\n",
      "          [ 0.1908, -0.2901,  0.2090],\n",
      "          [-0.2438, -0.0005, -0.2065]]],\n",
      "\n",
      "\n",
      "        [[[-0.2115,  0.1457, -0.1816],\n",
      "          [-0.2924,  0.3199, -0.0570],\n",
      "          [-0.1252,  0.0150, -0.1796]]],\n",
      "\n",
      "\n",
      "        [[[-0.0733,  0.2871, -0.1806],\n",
      "          [-0.0710, -0.0960,  0.0983],\n",
      "          [-0.2175, -0.1457,  0.1222]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1849, -0.2970, -0.3010],\n",
      "          [ 0.1445,  0.2162, -0.1427],\n",
      "          [ 0.0855,  0.0561,  0.1802]]],\n",
      "\n",
      "\n",
      "        [[[-0.2003,  0.3153, -0.2129],\n",
      "          [ 0.0997,  0.2821, -0.0877],\n",
      "          [-0.0367, -0.2310, -0.0435]]],\n",
      "\n",
      "\n",
      "        [[[-0.1330,  0.2927, -0.0996],\n",
      "          [ 0.1115, -0.0617,  0.2026],\n",
      "          [ 0.0854,  0.1284, -0.3189]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0394, -0.0737,  0.2335],\n",
      "          [ 0.0315,  0.1024,  0.0191],\n",
      "          [ 0.1007,  0.2212, -0.0246]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0097, -0.3317,  0.0495],\n",
      "          [ 0.0605,  0.2045,  0.1617],\n",
      "          [-0.2588,  0.2382,  0.1020]]],\n",
      "\n",
      "\n",
      "        [[[-0.2911,  0.2101, -0.0863],\n",
      "          [-0.1851,  0.2725,  0.1455],\n",
      "          [-0.0712,  0.0146, -0.1010]]],\n",
      "\n",
      "\n",
      "        [[[-0.0280, -0.1059,  0.1672],\n",
      "          [ 0.0161, -0.0418, -0.0293],\n",
      "          [-0.2915,  0.3177,  0.1607]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1499,  0.0744, -0.0609],\n",
      "          [ 0.2741,  0.2634, -0.3023],\n",
      "          [ 0.2327, -0.2497, -0.1559]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1928,  0.2180,  0.0317],\n",
      "          [-0.1215,  0.2550, -0.1272],\n",
      "          [-0.1832,  0.1956,  0.1641]]],\n",
      "\n",
      "\n",
      "        [[[-0.0098, -0.1728, -0.1748],\n",
      "          [ 0.3310, -0.2141,  0.1293],\n",
      "          [-0.0609,  0.2041, -0.0072]]],\n",
      "\n",
      "\n",
      "        [[[-0.2827, -0.1463, -0.2361],\n",
      "          [-0.0720,  0.1963, -0.3078],\n",
      "          [ 0.1393, -0.0894,  0.2399]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1822, -0.1961,  0.3163],\n",
      "          [ 0.1461,  0.2640, -0.2712],\n",
      "          [-0.2681, -0.1732, -0.2507]]],\n",
      "\n",
      "\n",
      "        [[[-0.3288,  0.1626, -0.3206],\n",
      "          [ 0.0645, -0.2031,  0.0248],\n",
      "          [ 0.1724,  0.1508, -0.0245]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0473,  0.1761,  0.2205],\n",
      "          [ 0.1047, -0.2445, -0.3073],\n",
      "          [ 0.1859,  0.2633, -0.1064]]],\n",
      "\n",
      "\n",
      "        [[[-0.2603,  0.1408,  0.1599],\n",
      "          [ 0.0552,  0.1244,  0.0027],\n",
      "          [-0.0082, -0.0621, -0.2353]]],\n",
      "\n",
      "\n",
      "        [[[-0.1342, -0.2685,  0.2826],\n",
      "          [ 0.3174, -0.1614, -0.0045],\n",
      "          [-0.0831, -0.0713, -0.2345]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0136,  0.1717, -0.2375],\n",
      "          [ 0.3174,  0.3134,  0.0546],\n",
      "          [-0.1241, -0.0263, -0.0861]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2216,  0.2891, -0.0102],\n",
      "          [-0.0579,  0.1079,  0.0554],\n",
      "          [ 0.2561,  0.0748, -0.0820]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0668,  0.1625,  0.1169],\n",
      "          [ 0.1584,  0.2911,  0.1805],\n",
      "          [ 0.0081,  0.3097, -0.1904]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1527,  0.2572, -0.2060],\n",
      "          [ 0.3115, -0.3122, -0.1887],\n",
      "          [-0.0953, -0.0323, -0.1646]]],\n",
      "\n",
      "\n",
      "        [[[-0.0800, -0.3182, -0.2164],\n",
      "          [-0.2183,  0.2814, -0.0991],\n",
      "          [-0.0655, -0.3143, -0.3171]]]], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CHEN_X~1\\AppData\\Local\\Temp/ipykernel_13504/793867865.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsi_extractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "for param in csi_extractor.parameters():\n",
    "    print(param)\n",
    "    print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
