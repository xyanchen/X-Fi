MI_model.py:
Direct concate features (with different num of features) from different modality

MI_model_1.py:
Map the different num of features from each modality to 32 (except mmwave)
if Lidar is TRUE, perform farthest point sampling to raw Lidar point cloud data to generate positional embedding

MI_model_2.py:
Change the mmwave feature extractor model structure (as in mmwave_point_transformer_TD.py). 
Sample num of features to 32 by using transition down layer after the first transformer layer.
In MI model, we cam map the different num of features from all modalities to 32.
MOE + query fusion -> can not converge

MI_model_3.py:
Modified from model 2. Directly project each modality feature to corresponding qkv. Based on the number of possible
modlities, set # of transformers that are working parallel.
Before sending to transformers, random select a modality as dominant modality and replace the query of all the other
modalities by dominant modality query (query fusion).
In each iteration, the modalities combination is different. Thus, we trigger different modality-corresponded transformers
by using "modality-existance-list". The active transformers form a transformer block. The output of each modality transformer
will be sumed up to generate the output of the transformer block, which will serve as the domiant query that send to the next
transformer block.
I tried use # of depth of none-identical transformer block, but could not converge.
Then I tried using shared weights transformer block and iterativly input the updated dominant query. It could converge slowly 
(took 150 epoches) and converged result is not good

MI_model_5.py:
See diagram

MI_model_6.py:
change the iteration on same block in the diagram to iteration on different blocks.